{"nbformat_minor": 2, "cells": [{"source": "# Basics of Spark on HDInsight\n\n<a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. When you provision a Spark cluster in HDInsight, you provision Azure compute resources with Spark installed and configured. The data to be processed is stored in Azure Blob storage (WASB).\n\n![Spark on HDInsight](https://mysstorage.blob.core.windows.net/notebookimages/overview/SparkArchitecture.png \"Spark on HDInsight\")", "cell_type": "markdown", "metadata": {}}, {"source": "Now that you have created a Spark cluster, let us understand some basics of working with Spark on HDInsight. For detailed discussion on working with Spark, see [Spark Programming Guide](https://spark.apache.org/docs/2.0.0/programming-guide.html).", "cell_type": "markdown", "metadata": {}}, {"source": "----------\n## Notebook setup\n\nWhen using Spark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; a SparkSession which has the SparkContext is created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n- SparkSession (spark)\n\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.\n\nEverytime you run a cell, your web browser window title will show a **(Busy)** status along with the notebook title. You will also see a solid circle next to the **Spark** text in the top-right corner. After the job completes, this will change to a hollow circle.\n\n![Status of a Jupyter notebook job](https://mysstorage.blob.core.windows.net/notebookimages/overview/HDI.Spark.Jupyter.Job.Status.Spark.Kernel.png \"Status of a Jupyter notebook job\")", "cell_type": "markdown", "metadata": {}}, {"source": "----\n## Configure notebook to use Azure Cosmos DB Spark Connector", "cell_type": "markdown", "metadata": {}}, {"source": "The Spark community contributes a lot of packages that extend Spark. These packages might not be available out of the box in the Spark distribution that you are using. Here is an example of how to use **spark-csv**, a CSV data source for Spark, in a notebook using the `%%configure` magic.\n\nPrior to executing any code, you will first need to include the Azure DocumentDB Spark JAR and CosmosDB Jar. You can use the below 'spark magic' command: ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"jars\": [\"wasb:///example/jars/azure-documentdb-1.14.0.jar\",\"wasb:///example/jars/azure-cosmosdb-spark_2.1.0_2.11-1.0.0.jar\"],\n  \"conf\": {\n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///example/jars/azure-documentdb-1.14.0.jar', u'wasb:///example/jars/azure-cosmosdb-spark_2.1.0_2.11-1.0.0.jar'], u'kind': 'spark', u'conf': {u'spark.jars.excludes': u'org.scala-lang:scala-reflect'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "----\n## Connecting Spark to Cosmos DB via the azure-cosmosdb-spark", "cell_type": "markdown", "metadata": {}}, {"source": "While the communication transport is a little more complicated, executing a query from Spark to Cosmos DB using 'azure-cosmosdb-spark' is significantly faster.\n\nBelow is a code snippet on how to use 'azure-cosmosdb-spark' within a Spark context.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "// Import Necessary Libraries\nimport org.joda.time._\nimport org.joda.time.format._\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark._\nimport com.microsoft.azure.cosmosdb.spark.config.Config\nimport org.apache.spark.sql.SaveMode\n\n// Configure connection to your collection\nval cosmosDBReadConfig = Config(Map(\"Endpoint\" -> \"https://contosoair-fight-record.documents.azure.com:443/\",\n\"Masterkey\" -> \"I6lQxqgIdejcmdxYcj70l6bGDPVb5c2qbs8zrS6OpmjNVZ02FZnS9ffKifAwuzRK9kPYFn9Ib9ohoNnen0CDhA==\",\n\"Database\" -> \"contosoair-flight-record\",\n\"preferredRegions\" -> \"West US;East US\",\n\"Collection\" -> \"flight-records\", \n\"SamplingRatio\" -> \"1.0\"))\n \n// Create temporary table \nval coll = spark.sqlContext.read.cosmosDB(cosmosDBReadConfig)\ncoll.createOrReplaceTempView(\"flightTemp\")\n\n// Create global table\nval sqlDF = spark.sql(\"SELECT Year, Quarter, Month, AirlineID, Carrier, OriginAirportID, Origin, OriginCityName, OriginState, OriginStateName, DestAirportID, Dest, DestCityName, DestState, DestStateName, CRSDepTime, DepTime, DepDelay, DepartureDelayGroups, CRSArrTime, ArrTime, ArrDelay, ArrivalDelayGroups, Cancelled, CancellationCode, Diverted, Distance, DistanceGroup, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay FROM flightTemp\")\nsqlDF.write.mode(SaveMode.Overwrite).saveAsTable(\"flightDB\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1511328301925_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.23kitdeysxtepcskukopafkvpa.dx.internal.cloudapp.net:8088/proxy/application_1511328301925_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.8:30060/node/containerlogs/container_1511328301925_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}